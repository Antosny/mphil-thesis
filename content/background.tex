\chapter{Background}
\label{chp:bg}

In this chapter, we would like to give a brief review of the related literatures.
We classify our work to be most related to the works in the areas of collaborative filtering.

In Table~\ref{tbl:relatedW}, we summarize the related works under the collaborative filtering context.
To the best of our knowledge, no previous work for collaborative filtering has ever focused on the fine-grained analysis of knowledge transfer between source domains and the target domain, i.e., the selective transfer learning.

In the following, we would like to discuss the state-of-the-art methods for both Collaborative Filtering and Transfer Learning.


\begin{table}[h]
\caption{Overview of STLCF in a big picture of Collaborative Filtering.}
\begin{Large}
\label{tbl:relatedW}
\begin{center}
\begin{tabular}{ c || c | c }
\hline\hline
& Selective & Non-Selective \\
\hline\hline
\multirow{1}{*} {Transfer} & \textbf{\bf \em STLCF} & RMGM~\cite{/ijcai/libin09}, CMF~\cite{/kdd/SinghG08},\\
\multirow{1}{*} {Learning} & & TIF~\cite{/aaai/WPan12}, etc.\\
\cline{1-3}
\multirow{1}{*} {Non-Transfer} & -- & MMMF~\cite{rennie2005fast}, GPLSA~\cite{DBLP:conf/sigir/Hofmann03}, \\
\multirow{1}{*} {Learning} &  & PMF~\cite{/nips/SalakhutdinovM07}, etc.\\
\hline\hline
\end{tabular}
\end{center}
\end{Large}
\end{table}

\hspace{0.1in}
\section{Collaborative Filtering}
As an intelligent component in recommendation systems, Collaborative Filtering (CF) has gained extensive interest in both academia and industry.
Generally speaking, CF is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that, if a person A has the same opinion as B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a randomly chosen person. For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of this user's tastes (likes or dislikes, ratings, etc).

There are three types of CF: memory-based, model-based and hybrid.

\hspace{0.05in}
\subsection{Memory-based CF}
This mechanism uses user rating data to compute the similarity between users or items. The similarity is then used for making recommendations. The memory-based method is used in many commercial systems, because it is easy to implement and is effective given plenty of records. Typical examples of this mechanism are neighborhood based CF and item-based/user-based top-N recommendations\cite{su2009survey}.

The advantages of this approach include:
\begin{itemize}
\item The explainability of the results, which is an important aspect of recommendation systems.
\item It is easy to setup and use.
\item New data can be added easily and incrementally.
\item It need not consider contents of the items being recommended.
\item The mechanism scales well with co-rated items.
\end{itemize}

However, there are several disadvantages with this approach:
\begin{itemize}
\item It requires plenty of human ratings.
\item Its performance decreases when data gets sparse, which is a common phenomenon with web related items.
\item Although it can efficiently handle new users, adding new items becomes more complicated since that representation usually relies on a specific vector space. That would require to include the new item and re-insert all the elements in the structure. This prevents the scalability of this approach.
\end{itemize}

\hspace{0.05in}
\subsection{Model-based CF}
Models are developed using data mining, machine learning algorithms to find patterns based on training data. This approach has a more holistic goal to uncover latent factors that explain observed ratings. Most of the models are based on creating a classification or clustering technique to identify the users in the test set.
Various models have been proposed, including factorization models~\cite{/computer/yehuda09matrix, /aaai/WPan12,paterek07,/tist/LibFM-TIST12},
probabilistic mixture models~\cite{hofmann04cf,jin:decoupled}, Bayesian networks~\cite{pennock00pd} and restricted Boltzman machines~\cite{/icml/SalakhutdinovMH07}.

There are several advantages with this paradigm:
\begin{itemize}
\item It handles the sparsity better than memory based ones.
\item This helps with scalability with large data sets.
\item It improves the prediction performance.
\item It gives an intuitive rationale for the recommendations.
\end{itemize}

The disadvantage of this approach is the expensive model building. On the one hand, the modern recommendation system usually have petabytes of records as input; On the other hand, the convergence of most models requires intensive computation. One needs to have a tradeoff between prediction performance and scalability.

Given the accuracy of model-based CF, how to overcome the scalability issue has attracted much concern. With the rapid development of parallel computation, researchers have been exploring the use of parallel system to speed up the complex model building. For example in ~\cite{chu2007map}, the authors showed that a variety of machine learning algorithms including k-means, logistic regression, naive Bayes, SVM, PCA, gaussian discriminant analysis, EM and backpropagation (NN) could be speeded up by Google's map-reduce ~\cite{dean2008mapreduce} paradigm.

Being aware of the computational infeasibility for most useful CF models in the real world settings, we would like to adopt the parallel computation framework in our implementation and experiments. We classified our Selective Transfer Learning for CF as model-based CF and use parallel computing to well handle the real world data and complex modeling.

\hspace{0.05in}
\subsection{Hybrid models}
A number of applications \cite{das2007google} combine the memory-based and the model-based CF algorithms. These overcome the limitations of native CF approaches. It improves the prediction performance. Importantly, it overcomes the CF problems such as sparsity and loss of information. However, they have increased complexity and are expensive to implement.

\hspace{0.1in}
\section{Transfer Learning} Pan and Yang ~\cite{/tkde/sinno09survey} surveyed the field of transfer learning. A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling effort.

Recently, researchers propose the MultiSourceTrAdaBoost~\cite{/cvpr/YaoD10} to allow automatically selecting the appropriate data for knowledge transfer from multiple sources. The newest work TransferBoost~\cite{/aaai/Eatond11} was proposed to iteratively construct an ensemble of classifiers via re-weighting source and target instance via both individual and task-based boosting. Moreover, EBBoost~\cite{/jmlr/ShivaswamyJ10a} suggests weight the instance based on the empirical error as well as its variance.

\hspace{0.05in}
\subsection{Transfer Learning for Collaborative Filtering}
Some works on transfer learning are in the context of collaborative filtering.
Mehta and Hofmann~\cite{/ki/bhaskar06cross} consider the scenario involving two systems with shared users and use manifold alignment methods to jointly build neighborhood models for the two systems. They focus on making use of an auxiliary recommender system when only part of the users are aligned, which does not distinguish the consistency of users' preferences among the aligned users.
Li \etal~\cite{/icml/libin09} designed a regularization framework to transfer knowledge of cluster-level rating patterns, which does not make use of the correspondence between the source and the target domains.

Our work is the first to systematically study {\em selective} knowledge transfer in the settings of collaborative filtering. Besides, we propose the novel factor - variance empirical error that is shown to be of much help in solving the real world CF problems.

\hspace{0.05in}
\subsection{Negative Transfer}
Negative transfer happens when the source domain data and task contribute to the reduced performance of learning in the target domain.
This could happen in the context of recommendation system. In the traditional transfer learning framework, if two users have common interests on the light music, we tend to believe they share the similar opinions on books. However, two users who have common interests on the light music may have quite different tastes on the books. In this case, if the transfer learning techniques are applied anyway, we can expect the bad performance. We call this phenomenon negative transfer.

Despite the fact that how to avoid negative transfer is a very important issue, little research work has been published on this topic. Rosenstein et al. \cite{rosenstein2005transfer} empirically showed that if two tasks are too dissimilar, brute-force transfer may hurt the performance of the target task. Some works have been exploited to analyze relatedness among tasks and task clustering techniques, such as ~\cite{ben2003exploiting} and ~\cite{bakker2003task}, which may help provide guidance on how to avoid negative transfer automatically. Bakker and Heskes ~\cite{bakker2003task} adopted a Bayesian approach in which some of the model parameters are shared for all the tasks and others more loosely connected through a joint prior distribution that can be learned from the data. Thus, the data are clustered based on the task parameters, where tasks in the same cluster are
supposed to be related to each other. Argyriou et al. \cite{argyriou2008algorithm} considered situations in which the learning tasks can be divided into groups. Tasks within each group are related by sharing a low-dimensional representation, which differs among different groups. As a result, tasks within a group can find it easier to transfer useful knowledge.

In our work, we investigate the negative transfer issue in the context of the recommendation system scenario. The key is to identify the items which would cause either much uncertainty in knowledge transfer or reduction in performance. Subsequently, we proposed a selective transfer learning framework for the model-based collaborative filtering tasks.

\hspace{0.05in}
\subsection{Large Scale Transfer Learning}
So far, transfer learning has been mostly considered in the off-line learning settings, which do not emphasize the scalability and computation speed. Due to the rapid development of storage technique and flourish of internet services, the real world problems in recent recommendation systems are mostly based on some large data sets. Little work on large scale transfer learning has been published in previous literature, though it is badly desirable. To cope with the growing needs of today's recommendation system, we would like to investigate the parallel framework in our experiments. There are already some researchers working on the large scale machine learning, as you may find in a post from quora \footnote{\url{http://www.quora.com/What-are-some-software-libraries-for-large-scale-learning}}. In our approach, we tried the Map-Reduce Framework and the Message Passing Interface (MPI).

\hspace{0.02in}
\subsubsection{Map-Reduce Framework}
MapReduce is a framework for processing parallelizable problems in huge datasets using a large number of computers (nodes). A MapReduce program comprises a Map() procedure that performs filtering and sorting (such as sorting students by first name into queues, one queue for each name) and a Reduce() procedure that performs a summary operation (such as counting the number of students in each queue, yielding name frequencies).
\begin{itemize}
\item {\bf ``Map" step:} The master node takes the input, divides it into smaller sub-problems, and distributes them to worker nodes. A worker node may do this again in turn, leading to a multi-level tree structure. The worker node processes the smaller problem, and passes the answer back to its master node.
\item {\bf ``Reduce" step:} The master node then collects the answers to all the sub-problems and combines them in some way to form the output, i.e. the answer to the problem it was originally trying to solve.
\end{itemize}
We will show that our methods can be plugged into the Map-Reduce framework for parallelization.

\hspace{0.02in}
\subsubsection{Message Passing Interface}
Message Passing Interface (MPI) is a standardized and portable message-passing system designed by a group of researchers from academia and industry to function on a wide variety of parallel computers. Both point-to-point and collective communication are supported. As a dominant model used in high-performance computing, MPI's goals are high performance, scalability, and portability. There are several advantages of MPI:
\begin{itemize}
\item {\bf Universality.} The message-passing model fits well on separate processors connected by a either fast or slow communication network.
\item {\bf Expressivity.} MPI exposes powerful interface to express the data parallel models.
\item {\bf Ease of debugging.} Because we write shared-memory (machine learning) models under MPI, the debugging process is relatively easier.
\item {\bf Performance.} As modern CPUs have become faster, management of their caches and the memory hierarchy has become the key to getting the most out of the machines. Message passing provides a way for the programmer to explicitly associate specific data with processers and thus allow the compiler and cache-management hardware to function fully.
\end{itemize}
Due to the flexibility of the protocol provided under MPI, it works well for the code development of learning algorithms. We will use MPI for implementation.
