\chapter{Conclusion and Future work}
\label{chp:conclusion}

In this thesis, we proposed to perform knowledge transfer for one-class CF problems using matrix factorization and came up with a matrix tri-factorization method and an online framework to systematically study on the factors that will affect selection.
We found although there exist some methods which tackle the one-class CF problem and there are also some transfer learning methods for CF. But no one has deal with one-class CF in multiple domains. Simply apply former transfer learning methods will fail due to data sparsity. 
We found under matrix tri-factorization framework(TRIMF), we can transfer as much knowledge as we can while ignore the noise. 
By leveraging overlapped users and items, we can transfer knowledge from different domains. While applying the linear control factor to pattern matrix, we can avoid direct transfer which can bring noise while capture the similarity between different domains.
To put our method in reality, we developed a clustering based matrix factorization framework(CBMF) which automatically integrate all data together then perform matrix factorization.
The experimental results for TRIMF in real-world data sets showed that our method performs better than several state-of-the-art methods in conversion rate comparison.
The experimental results for CBMF in real-world showed that our method has the best conversion rate and moderate click-through rate among others.
 
However, we notice that there are limitations in the work. First, in TRIMF, the computational cost is expensive since multiplicative rules will affect all matrix in update time. Second, we only support non-negative matrix factorization in TRIMF, because we need to constrain non-negative to fulfill optimization conditions. If matrix can be negative, it'll be more flexible and can carry more information. Third, both TRIMF and CBMF are point-wise methods which optimize for each entry of the matrix, actually we only need to rank those items not to calculate their score. That is, we only need their relative relationship \cite{Rendle:2009:BBP:1795114.1795167}. Fourth, TRIMF and CBMF are batch updated algorithms, but in online test almost all algorithms whose performance are good are real-time.


We believe that Transfer Learning for One-Class Recommendation has practical applications in the real world and would be a promising research topic. TRIMF/CBMF represents our initial attempt on this topic. In the future to make it more robust, we propose the following approaches:
\begin{itemize}
\item {\bf Pair-wise Transfer Learning in CF.} Instead of point-wise transfer in CF, pair-wise CF is becoming more and more popular because it can almost achieve better results. In \cite{DBLP:dblp_conf/recsys/LercheJ14, DBLP:dblp_conf/recsys/Aiolli14} pair-wise CF is applied in implicit feedback. In transfer learning, integrating matrix factorization and pair-wise CF can be the future work.
\item {\bf Online Transfer Learning in CF.} There are little work on large scale transfer learning, but it is badly desirable. In real-world, online recommendation algorithms often dominant off-line ones. Our method CBMF is a batch-updating algorithm which updates per hour, but not real-time. It would be our future work to make a real online transfer learning algorithm in CF.
\item {\bf Transfer Learning in CF with multiple matrix.} In CBMF, data from different sources are integrated into one unify matrix. Although very carefully, we can still lose or misuse the data. If we can run our algorithm fast on their original data, then we don't need to integrate.
\item {\bf Time Complexity Optimization in CF.} In \cite{Shalev-Shwartz:2008:SOI:1390156.1390273}, an interesting relationship is shown: more data can faster training speed while getting the same performance on test data. In CF there are many data that we need plenty of time to deal with. If we could leverage all of them without increasing our training time or model complexity, we could use as much data as possible.
\end{itemize} 
