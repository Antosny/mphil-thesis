\chapter{Conclusion and Future work}
\label{chp:conclusion}
In this thesis, we proposed to perform knowledge transfer for one-class of CF problems using matrix factorization and came up with a matrix tri-factorization method and an online framework to systematically study the factors that will affect selection. We found although there exists some methods which tackle the one-class CF problem and there are also some transfer learning methods for CF, however, no one has dealt with one-class CF in multiple domains. Simply applying former transfer learning methods will fail due to data sparsity. We found under the matrix tri-factorization framework (TRIMF), we can transfer as much knowledge as we can while ignoring any noise. Through leveraging overlapping users and items, we can transfer knowledge from different domains. While applying the linear control factor to a pattern matrix, we can avoid a direct transfer, which can bring noise, while capturing the similarity between different domains. To bring our method into reality, we developed a clustering based matrix factorization framework (CBMF) which automatically integrates all data together then performs matrix factorization. The experimental results for TRIMF in real-world data sets showed that our method performs better than several state-of-the-art methods in conversion rate comparison. The experimental results for CBMF in real-world situations showed that our method has the best conversion rate and moderate click-through rate than the others. 
 
However, we notice that there are limitations in the work. First, in TRIMF, the computational cost is high since multiplicative rules will affect all matrices in update time. Second, we only support non-negative matrix factorization in TRIMF, because we need non-negative constraints to fulfill optimization conditions. If a matrix can be negative, it will be more flexible and carry more information. Third, both TRIMF and CBMF are point-wise methods which optimize each entry of the matrix, and we actually only need to rank those items not calculate their score. That is, we only need their relative relationship \cite{Rendle:2009:BBP:1795114.1795167}. Fourth, TRIMF and CBMF are batch updated algorithms, but in online tests almost all algorithms whose performances are good are real-time.

We believe that Transfer Learning for One-Class Recommendation has practical applications in the real world and would be a promising research topic. TRIMF/CBMF represents our initial attempt on this topic. In the future to make it more robust, we propose the following approaches:
\begin{itemize}
\item {\bf Pair-wise Transfer Learning in CF.} Instead of point-wise transfer in CF, pair-wise CF is becoming more and more popular because it almost achieves better results. In \cite{DBLP:dblp_conf/recsys/LercheJ14, DBLP:dblp_conf/recsys/Aiolli14} pair-wise CF is applied in implicit feedback. In transfer learning, integrating matrix factorization and pair-wise CF can be future work.
\item {\bf Online Transfer Learning in CF.} There is little work on large scale transfer learning, but it is highly desirable. In real-world, online recommendation algorithms often dominant off-line ones. Our method CBMF is a batch-updating algorithm which updates every hour, but not in real-time. To make a real online transfer learning algorithm in CF would be our future work.
\item {\bf Transfer Learning in CF with multiple matrix.} In CBMF, data from different sources are integrated into one unify matrix, although it must be done carefully as we could still lose or misuse the data. If we can run our algorithm quickly on the original data, then we do not need to integrate it.
\item {\bf Time Complexity Optimization in CF.} In \cite{Shalev-Shwartz:2008:SOI:1390156.1390273}, an interesting relationship is shown: more data can train at a faster speed while getting the same performance on the test data. If we could leverage all of them without increasing our training time or model complexity, we could use as much data as possible.
\end{itemize} 
